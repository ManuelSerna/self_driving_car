# -*- coding: utf-8 -*-
"""traffic_signs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jFn5LATobwo9CYq3EIjL1_okVNviGwXl
"""

# Notebook: use UNIX shell commands to obtain data from repository
!git clone https://bitbucket.org/jadslim/german-traffic-signs

!ls german-traffic-signs

#*************************************************************************
'''
Classifying Road Symbols with a convolutional neural network.

* Note: This project was originally created on a python notebook (Google Colaboratory). 
This was done to take advantage of the free GPU power offered, and getting data from the repository service bitbucket.
'''
#*************************************************************************
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
from keras.layers import Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D

# Pickle: serializes images
import pickle

# manipulate and analyze data inside of a csv file (comma-separated-files)
import pandas as pd

# We will need to reduce the color channels to one (greyscale) so that our neural network can process the images with less difficulty
import cv2

# Generate altered images when requested
from keras.preprocessing.image import ImageDataGenerator

import requests
from PIL import Image

import random
np.random.seed(0)

#*************************************************************************
# Preprocess images in several steps
#*************************************************************************

# Unpickle contents of f into each respective data set
with open('german-traffic-signs/train.p', 'rb') as f:
    train_data = pickle.load(f) 
with open('german-traffic-signs/valid.p', 'rb') as f:
    val_data = pickle.load(f)
with open('german-traffic-signs/test.p', 'rb') as f:
    test_data = pickle.load(f)
  
print(type(train_data))
X_train, y_train = train_data['features'], train_data['labels']
X_val, y_val = val_data['features'], val_data['labels']
X_test, y_test = test_data['features'], test_data['labels']

# Visualize size and dimensions (32x32) of data sets, the traffic signs have 3 color channels (RGB). 
# At this point we have effectively imported our data.
print('training data: ', X_train.shape)
print('validation data: ', X_val.shape)
print('testing data: ', X_test.shape)

# Assert the number of images equals the number of labels
assert(X_train.shape[0] == y_train.shape[0]), 'The number of images is not equal to the number of labels'
assert(X_val.shape[0] == y_val.shape[0]), 'The number of images is not equal to the number of labels'
assert(X_test.shape[0] == y_test.shape[0]), 'The number of images is not equal to the number of labels'
# Assert that the training image at index 1 and beyond is 32x32x3
assert(X_train.shape[1:] == (32, 32, 3)), "The dimensions of the images are not 32 x 32 x 3"
assert(X_val.shape[1:] == (32, 32, 3)), "The dimensions of the images are not 32 x 32 x 3"
assert(X_test.shape[1:] == (32, 32, 3)), "The dimensions of the images are not 32 x 32 x 3"

# Load from csv file
data = pd.read_csv('german-traffic-signs/signnames.csv')
#print(data)

# Display selected images (randomly out their set) in a grid (without axis labels)
num_of_samples=[]
 
cols = 5
num_classes = 43

'''
# Display each class with several sample images
fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5,50))
fig.tight_layout()

# (index, Series)
for i in range(cols):
    for j, row in data.iterrows():
        x_selected = X_train[y_train == j]
        axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))
        axs[j][i].axis("off")
        if i == 2:
            axs[j][i].set_title(str(j) + '-' + row["SignName"])
            num_of_samples.append(len(x_selected))
'''
'''
Plot number of samples belonging to each class.
- As you will see, some classes have MUCH more images to train with than others.
(which is natural, some signs appear more than others)

print(num_of_samples)
plt.figure(figsize=(12, 4))
plt.bar(range(0, num_classes), num_of_samples)
plt.title("Distribution of the train dataset")
plt.xlabel("Class number")
plt.ylabel("Number of images")
plt.show()
'''

#=======================================================================
# Turn RGB images into greyscale.
#=======================================================================
def grayscale(img):
    grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    return grey

#=======================================================================
'''
Equalize grayscale intensities.
Histogram equalization: aims to standardize the lighting in all images.
'''
#=======================================================================
def equalize(img):
    equal = cv2.equalizeHist(img) # function will only accept grayscale images
    return equal

#==================================================================
# Preprocess entire data set.
#==================================================================
def preprocessing(img):
    img = grayscale(img)
    img = equalize(img)
    img = img/255 # step 3) normalize gray intensities
    return img # return preprocessed image

# Preprocess all data sets
X_train = np.array(list(map(preprocessing, X_train)))
X_val = np.array(list(map(preprocessing, X_val)))
X_test = np.array(list(map(preprocessing, X_test)))

# Format arrays as 32x32 images with a depth of 1 (for greyscale).
X_train = X_train.reshape(34799, 32, 32, 1)
X_val = X_val.reshape(4410, 32, 32, 1)
X_test = X_test.reshape(12630, 32, 32, 1)

# Produce augmented images
data_gen = ImageDataGenerator(width_shift_range=0.1, # shift image horizontally
                   height_shift_range=0.1, # shift image vertically
                   zoom_range=0.2, # zoom in as close as 1.2x or out as far as 0.8
                   shear_range=0.1,
                   rotation_range=10) # rotate image in degrees
data_gen.fit(X_train) # create new augmented images when requested, reducing memory requirements

'''
# Print shape to confirm arrays are now 1D
print('Shape of new training data: ', X_train.shape)
print('Shape of new validation data: ', X_val.shape)
print('Shape of new test data: ', X_test.shape)
'''

# One hot encode data labels
y_train = to_categorical(y_train, 43)
y_val = to_categorical(y_val, 43)
y_test = to_categorical(y_test, 43)

# Now we have properly preprocessed our images.

#=========================================================================
# Utilize modified leNet convolutional neural network.
#=========================================================================
def modified_model():
    model = Sequential()
    # Add layers, start with convolutional layer with 30 5x5 filters, we will not modify the stride and padding args
    '''
    EDIT: increase the number of filters to improve model.
    Can double the amount of filters in each layer, from 30 to 60 in the first layer. Then 15 to 30 in the second.
    * While increasing the number of filters increases the number of parameters and thus the amount of computing power needed,
    it is a necessary modification for improving our network's performance.
    
    EDIT: add more convolutional layers to improve performance. Can extract more features and can also lead to improved accuracy.
    * By adding more convolutional layers, the dimensions of our image decrease. By the time our image of data reaches the
    fully connected layers, it has much smaller dims, so less parameters, less overall parameters overall.
    '''
    model.add(Conv2D(60, (5, 5), input_shape=(32, 32, 1), activation='relu'))
    model.add(Conv2D(60, (5, 5), activation='relu')) # EDIT: added one more convo layer
    # Add pooling layer (scale down feature maps into a small, generalized representation, helps to avoid overfitting)
    model.add(MaxPooling2D(pool_size=(2, 2))) # (2,2), so scale down to half the size, same depth of 30
    
    # Add another convoltional layer
    model.add(Conv2D(30, (3, 3), activation='relu'))
    model.add(Conv2D(30, (3, 3), activation='relu')) # EDIT: added one more convo layer
    # Feed input into second pooling layer
    model.add(MaxPooling2D(pool_size=(2, 2)))
    '''
    EDIT: using more than one dropout layer is common and can be a very effective technique.
    - Further reduces overfitting.
    '''
    #model.add(Dropout(0.5))
    
    # Flatten array of inputs in order to format properly so that it can be fed into the fully-connected layer
    model.add(Flatten())
    model.add(Dense(500, activation='relu'))
    model.add(Dropout(0.5)) # drop half of all input nodes at each update
    # Define output layer
    model.add(Dense(num_classes, activation='softmax'))
    # Compile model
    '''
    EDIT: decreasing learning rate in Adam can help a nn learn more effectively when a more complex dataset is involed.
    Decrease learning rate from 0.1 to 0.001
    '''
    model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = modified_model()
print(model.summary())

# Train model using augmented data
# Flow: request image generator for images. Create a batch size of 20 we get 20 new images each time the next function is called on the iterator.
history = model.fit_generator(
    data_gen.flow(X_train, y_train, batch_size=50), 
    steps_per_epoch=2000, epochs=10, 
    validation_data=(X_val, y_val), 
    shuffle=1
    )
'''
# Plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('Loss')
plt.xlabel('epoch')
'''
'''
# Plot accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.legend(['training', 'validation'])
plt.title('Accuracy')
plt.xlabel('epoch')
'''

score = model.evaluate(X_test, y_test, verbose=0)
print('Test score: ', score[0])
print('Test accuracy: ', score[1])

'''
- We now have a quantitative understanding of our network's performance and it's not very good.
- Looking at the loss, it is even higher at a minimum value of about ~0.9.
- Validation accuracy seems to lag behind of training accuracy, which is at about 96%.

These values imply that our network is not performing effectively in terms of accurately predicting images from the dataset.
    - What's more our network seems to have overfit in our data as well becuase the validation acc trails behind the training acc.
    - We must finetune our model to improve performance.

* 2 Main issues:
    - Accuracy is not as high as we would like
    - Network seems to overfit the training data.

Always try to modify your model to see just how these modifications improve the effectiveness of your model.
'''

# Let's use images from the internet to put this modified cnn to the test
'''
https://c8.alamy.com/comp/G667W0/road-sign-speed-limit-30-kmh-zone-passau-bavaria-germany-G667W0.jpg

https://c8.alamy.com/comp/A0RX23/cars-and-automobiles-must-turn-left-ahead-sign-A0RX23.jpg

https://previews.123rf.com/images/bwylezich/bwylezich1608/bwylezich160800375/64914157-german-road-sign-slippery-road.jpg

https://previews.123rf.com/images/pejo/pejo0907/pejo090700003/5155701-german-traffic-sign-no-205-give-way.jpg

https://c8.alamy.com/comp/J2MRAJ/german-road-sign-bicycles-crossing-J2MRAJ.jpg
'''
# Fetch image
url = 'https://c8.alamy.com/comp/J2MRAJ/german-road-sign-bicycles-crossing-J2MRAJ.jpg'
r = requests.get(url, stream=True)
img = Image.open(r.raw)
plt.imshow(img, cmap=plt.get_cmap('gray'))

#Preprocess image
 
img = np.asarray(img)
img = cv2.resize(img, (32, 32))
img = preprocessing(img) # call previously-defined function
plt.imshow(img, cmap = plt.get_cmap('gray'))
print(img.shape)

# Reshape reshape
img = img.reshape(1, 32, 32, 1)

# Test image
print("predicted sign: "+ str(model.predict_classes(img)))
